{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming dataset to replace Named Entities\n",
    "\n",
    "Numerical and named entities are replaced by the corresponding Named entity tag. Each token is POS tagged and lemmatized.\n",
    "\n",
    "E.g:\n",
    "\n",
    "*Joan found 70 seashells on the beach . she gave Sam some of her seashells . She has 27 seashell . How many seashells did she give to Sam ?*\n",
    "\n",
    "will be converted to:\n",
    "\n",
    "*PERSON find NUMBER seashell on the beach . she give PERSON some of she seashell . she have NUMBER seashell . how many seashell do she give to PERSON ?\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycorenlp import StanfordCoreNLP\n",
    "import json\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "\n",
    "#names of the files\n",
    "\n",
    "'''\n",
    "\n",
    "addition problems dataset:\n",
    "add_1.json\n",
    "add_2.json\n",
    "add_3.json\n",
    "add_4.json\n",
    "\n",
    "subtraction problems:\n",
    "sub_1.json\n",
    "sub_2.json\n",
    "sub_3.json\n",
    "sub_4.json\n",
    "\n",
    "'''\n",
    "\n",
    "with open(\"data.json\",'r') as f:\n",
    "    dataset = json.load(f)\n",
    "    \n",
    "named = ['PERSON', 'LOCATION', 'ORGANIZATION', 'MISC']\n",
    "numerical = ['MONEY', 'NUMBER', 'ORDINAL', 'PERCENT']\n",
    "\n",
    "\n",
    "clean_problems = []\n",
    "\n",
    "count = 0\n",
    "\n",
    "for problem in dataset:\n",
    "\n",
    "    new_problem = {}\n",
    "    new_problem['iIndex'] = count\n",
    "    new_problem['sQuestion'] = ''\n",
    "    \n",
    "\n",
    "    prob = problem['sQuestion']\n",
    "    new_problem['origQuestion'] = prob\n",
    "    \n",
    "    res = nlp.annotate(prob, properties = {'annotators':\"tokenize,ssplit,pos,lemma,ner\", 'outputFormat':'json'})\n",
    "  \n",
    "    for j in range(len(res['sentences'])): # all sentences in the problem\n",
    "       \n",
    "        clean_sentence = []\n",
    "    \n",
    "        tokens = res['sentences'][j]['tokens']\n",
    "        \n",
    "        for i in range(len(tokens)): # all tokens in a sentence\n",
    "        \n",
    "            if tokens[i]['ner'] in named or tokens[i]['ner'] in numerical:\n",
    "                clean_sentence.append(tokens[i][\"ner\"])\n",
    "            \n",
    "            else:\n",
    "                clean_sentence.append(tokens[i][\"lemma\"]) \n",
    "        \n",
    "        new_problem['sQuestion'] += ' '+' '.join(clean_sentence)\n",
    "\n",
    "    clean_problems.append(new_problem)\n",
    "\n",
    "    count += 1\n",
    "\n",
    "    # writing result to file\n",
    "\n",
    "with open('clean_lemma_ner_data.json', 'w') as f:\n",
    "    json.dump(clean_problems, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
